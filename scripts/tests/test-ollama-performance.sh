#!/bin/bash
# scripts/test-ollama-performance.sh
# Script para probar el rendimiento de Ollama y validar las optimizaciones

set +e

echo "๐งช PRUEBAS DE RENDIMIENTO DE OLLAMA"
echo "===================================="
echo ""

# Colores para output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Verificar que Ollama estรก corriendo
if ! docker ps | grep -q "ollama.*healthy"; then
    echo "โ ERROR: Ollama no estรก corriendo o no estรก saludable"
    exit 0
fi

echo "โ Ollama estรก funcionando correctamente"
echo ""

# Funciรณn para medir tiempo de carga de modelo
test_model_load_time() {
    local model=$1
    echo -e "${BLUE}๐ฆ Probando carga de modelo: ${model}${NC}"
    
    # Limpiar modelo de memoria primero (si estรก cargado)
    docker exec ollama ollama show "$model" > /dev/null 2>&1 || true
    
    # Medir tiempo de carga
    start_time=$(date +%s.%N)
    docker exec ollama ollama run "$model" "test" > /dev/null 2>&1
    end_time=$(date +%s.%N)
    
    load_time=$(echo "$end_time - $start_time" | bc)
    echo -e "${GREEN}   โฑ๏ธ  Tiempo de carga: ${load_time}s${NC}"
    echo "$load_time"
}

# Funciรณn para medir velocidad de inferencia
test_inference_speed() {
    local model=$1
    local prompt="Write a short story about a robot learning to paint. Make it exactly 100 words."
    
    echo -e "${BLUE}๐ Probando velocidad de inferencia: ${model}${NC}"
    
    # Ejecutar inferencia y medir tiempo
    start_time=$(date +%s.%N)
    response=$(docker exec ollama ollama run "$model" "$prompt" 2>/dev/null)
    end_time=$(date +%s.%N)
    
    inference_time=$(echo "$end_time - $start_time" | bc)
    word_count=$(echo "$response" | wc -w)
    
    if [ "$word_count" -gt 0 ]; then
        words_per_second=$(echo "scale=2; $word_count / $inference_time" | bc)
        echo -e "${GREEN}   โฑ๏ธ  Tiempo total: ${inference_time}s${NC}"
        echo -e "${GREEN}   ๐ Palabras generadas: ${word_count}${NC}"
        echo -e "${GREEN}   ๐ Velocidad: ${words_per_second} palabras/segundo${NC}"
    else
        echo -e "${YELLOW}   โ๏ธ  No se generaron palabras${NC}"
    fi
    
    echo ""
}

# Funciรณn para probar modelo en cache (segunda carga)
test_cached_model_load() {
    local model=$1
    echo -e "${BLUE}๐พ Probando carga desde cache: ${model}${NC}"
    
    # El modelo ya deberรญa estar en memoria
    start_time=$(date +%s.%N)
    docker exec ollama ollama run "$model" "test" > /dev/null 2>&1
    end_time=$(date +%s.%N)
    
    cached_load_time=$(echo "$end_time - $start_time" | bc)
    echo -e "${GREEN}   โฑ๏ธ  Tiempo de carga desde cache: ${cached_load_time}s${NC}"
    echo "$cached_load_time"
}

# Funciรณn para mostrar uso de recursos
show_resource_usage() {
    echo -e "${BLUE}๐ Uso de recursos actual:${NC}"
    docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}" | grep -E "NAME|ollama"
    echo ""
}

# Funciรณn para mostrar modelos disponibles
show_available_models() {
    echo -e "${BLUE}๐ Modelos disponibles:${NC}"
    docker exec ollama ollama list 2>/dev/null | head -10
    echo ""
}

# Funciรณn principal de pruebas
main() {
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo "PASO 1: Verificar estado y modelos disponibles"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo ""
    
    show_available_models
    show_resource_usage
    
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo "PASO 2: Prueba de carga de modelos (primera vez)"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo ""
    
    # Probar con un modelo pequeรฑo primero
    echo "Probando con modelo pequeรฑo (all-minilm)..."
    test_model_load_time "all-minilm:latest"
    echo ""
    
    # Probar con un modelo mediano
    echo "Probando con modelo mediano (deepseek-r1:14b)..."
    test_model_load_time "deepseek-r1:14b"
    echo ""
    
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo "PASO 3: Prueba de velocidad de inferencia"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo ""
    
    test_inference_speed "deepseek-r1:14b"
    
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo "PASO 4: Prueba de carga desde cache (segunda vez)"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo ""
    
    echo "El modelo deberรญa estar en memoria (OLLAMA_KEEP_ALIVE=10m)..."
    cached_time=$(test_cached_model_load "deepseek-r1:14b")
    echo ""
    
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo "PASO 5: Verificar configuraciรณn de optimizaciones"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo ""
    
    echo -e "${BLUE}Variables de entorno aplicadas:${NC}"
    docker exec ollama env 2>/dev/null | grep -E "OLLAMA_(MAX_LOADED_MODELS|NUM_THREAD|KEEP_ALIVE)" | sort
    echo ""
    
    echo -e "${BLUE}Shared Memory Size:${NC}"
    docker inspect ollama 2>/dev/null | grep -i "ShmSize" | awk '{print "   ShmSize: " $2 " bytes (" $2/1024/1024/1024 " GB)"}'
    echo ""
    
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo "โ PRUEBAS COMPLETADAS"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo ""
    echo "๐ก Interpretaciรณn de resultados:"
    echo "   โข Tiempo de carga inicial: deberรญa ser rรกpido (< 5s para modelos pequeรฑos)"
    echo "   โข Tiempo de carga desde cache: deberรญa ser muy rรกpido (< 1s)"
    echo "   โข Velocidad de inferencia: depende del modelo y GPU"
    echo "   โข Si la segunda carga es mucho mรกs rรกpida, el cache funciona correctamente"
    echo ""
}

# Ejecutar pruebas
main

