# ğŸ§ª GuÃ­a de Pruebas para Optimizaciones de Ollama

Esta guÃ­a describe cÃ³mo probar y validar las optimizaciones aplicadas a Ollama.

## ğŸ“‹ Pruebas Disponibles

### 1. Prueba RÃ¡pida (Recomendada)

Ejecuta el script de pruebas rÃ¡pidas:

```bash
./scripts/test-ollama-quick.sh
```

Este script verifica:
- âœ… ConfiguraciÃ³n de variables de entorno
- âœ… Shared Memory Size
- âœ… Tiempo de carga de modelos
- âœ… Funcionamiento del cache
- âœ… Velocidad de inferencia bÃ¡sica

### 2. Prueba Completa

Para pruebas mÃ¡s detalladas:

```bash
./scripts/test-ollama-performance.sh
```

**Nota**: Esta prueba puede tardar varios minutos ya que carga modelos grandes.

## ğŸ” Pruebas Manuales

### Prueba 1: Verificar ConfiguraciÃ³n

```bash
# Verificar variables de entorno
docker exec ollama env | grep OLLAMA

# Verificar Shared Memory Size
docker inspect ollama | grep ShmSize

# Verificar modelos disponibles
docker exec ollama ollama list
```

### Prueba 2: Tiempo de Carga de Modelos

```bash
# Cargar modelo pequeÃ±o (primera vez)
time docker exec ollama ollama run all-minilm:latest "test"

# Cargar el mismo modelo (segunda vez - desde cache)
time docker exec ollama ollama run all-minilm:latest "test"
```

**Resultado esperado**: La segunda carga deberÃ­a ser significativamente mÃ¡s rÃ¡pida (< 1s vs 2-5s).

### Prueba 3: Velocidad de Inferencia

```bash
# Probar con modelo pequeÃ±o
time docker exec ollama ollama run all-minilm:latest "Write a 50-word story about space"

# Probar con modelo mediano (si tienes GPU)
time docker exec ollama ollama run deepseek-r1:14b "Explain quantum computing in simple terms"
```

### Prueba 4: Uso de Recursos

```bash
# Monitorear uso de recursos durante inferencia
docker stats ollama

# En otra terminal, ejecutar inferencia
docker exec ollama ollama run deepseek-r1:14b "Write a long story"
```

### Prueba 5: Verificar Cache de Modelos

```bash
# Cargar modelo
docker exec ollama ollama run deepseek-r1:14b "test"

# Esperar 5 minutos (dentro del KEEP_ALIVE de 10m)
sleep 300

# Cargar de nuevo (deberÃ­a ser rÃ¡pido - desde cache)
time docker exec ollama ollama run deepseek-r1:14b "test"
```

## ğŸ“Š MÃ©tricas de GPU

Si tienes GPU NVIDIA, puedes monitorear el uso:

```bash
# Ver uso de GPU en tiempo real
watch -n 1 nvidia-smi

# Ver mÃ©tricas especÃ­ficas
nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv
```

## ğŸ“ˆ MÃ©tricas de Prometheus/Grafana

Las optimizaciones tambiÃ©n se pueden monitorear desde Grafana:

1. **Accede a Grafana**: http://localhost:3000
2. **Ve al dashboard**: "AI Models Performance Dashboard"
3. **Verifica mÃ©tricas**:
   - Ollama Status (deberÃ­a ser 1)
   - Total Models (nÃºmero de modelos disponibles)
   - Total Models Size (tamaÃ±o total de modelos)

## âœ… Criterios de Ã‰xito

Las optimizaciones estÃ¡n funcionando correctamente si:

1. âœ… **Variables de entorno aplicadas**: 
   - `OLLAMA_MAX_LOADED_MODELS=2`
   - `OLLAMA_NUM_THREAD=8`
   - `OLLAMA_KEEP_ALIVE=10m`

2. âœ… **Shared Memory Size**: 2GB (2147483648 bytes)

3. âœ… **Cache funcionando**: 
   - Segunda carga de modelo es > 50% mÃ¡s rÃ¡pida que la primera

4. âœ… **Rendimiento mejorado**:
   - Tiempo de carga inicial < 5s para modelos pequeÃ±os
   - Tiempo de carga desde cache < 1s
   - Velocidad de inferencia estable

5. âœ… **Recursos optimizados**:
   - Uso de CPU razonable (< 50% en idle)
   - Uso de memoria apropiado
   - GPU utilizada cuando hay modelos grandes cargados

## ğŸ”§ SoluciÃ³n de Problemas

### Problema: Modelos no se cargan mÃ¡s rÃ¡pido

**SoluciÃ³n**: Verifica que las variables de entorno estÃ©n aplicadas:
```bash
docker exec ollama env | grep OLLAMA_MAX_LOADED_MODELS
```

### Problema: Cache no funciona

**SoluciÃ³n**: Verifica `OLLAMA_KEEP_ALIVE`:
```bash
docker exec ollama env | grep OLLAMA_KEEP_ALIVE
```

### Problema: Shared Memory Size no aplicado

**SoluciÃ³n**: Reinicia el contenedor:
```bash
docker compose restart ollama-gpu
```

## ğŸ“ Notas

- Las pruebas pueden tardar varios minutos con modelos grandes
- Los tiempos pueden variar segÃºn el hardware
- El cache funciona mejor con modelos que se usan frecuentemente
- Las optimizaciones son mÃ¡s notables con modelos grandes (> 7B parÃ¡metros)

## ğŸ¯ PrÃ³ximos Pasos

DespuÃ©s de validar las optimizaciones:

1. **Monitorear uso real**: Usa Ollama normalmente y observa las mejoras
2. **Ajustar parÃ¡metros**: Si es necesario, ajusta valores en `.env`
3. **Continuar optimizando**: Considera Redis cache o mejoras de HAProxy

